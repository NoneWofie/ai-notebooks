{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Language Generator\n",
    "\n",
    "The input is broken down into a sequence of characters that are fed into our network one character at a time.\n",
    "\n",
    "Thus the model will process each new character in conjunction with the memory of previous seen characters to predict the next.\n",
    "\n",
    "<b>Steps:</b>\n",
    "1. Preparing the data\n",
    "2. Building the RNN\n",
    "3. Performing next-character prediction and sampling to generate new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1143k  100 1143k    0     0   337k      0  0:00:03  0:00:03 --:--:--  337k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://www.gutenberg.org/files/1268/1268-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1112300\n",
      "Unique Characters: 80\n"
     ]
    }
   ],
   "source": [
    "#Reading and processing text \n",
    "with open('1268-0.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_index = text.find('END OF THE PROJECT GUTENBERG')\n",
    "\n",
    "text = text[start_indx: end_index]\n",
    "\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE MYSTERIOUS ISLAND ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE MYSTERIOUS ISLAND\n",
      "\n",
      "by Jules Verne\n",
      "\n",
      "1874\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PART 1--DROPPED FROM T\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dic for word to int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1112300,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
      "[33 43 36 25 38 28]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "chars_sorted = sorted(char_set)\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "char_array = np.array(chars_sorted)\n",
    "\n",
    "# Contains the encoded values of all the chracters in the text\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(char_array[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 -> T\n",
      "32 -> H\n",
      "29 -> E\n",
      "1 ->  \n",
      "37 -> M\n"
     ]
    }
   ],
   "source": [
    "for ex in text_encoded[:5]:\n",
    "    print(f'{ex} -> {char_array[ex]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
      "  6  0  0  0  0  0 44 32 29  1 37 48 43 44 29 42]  ->  33\n",
      "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'  ->  'I'\n"
     ]
    }
   ],
   "source": [
    "#Total input sequence length = 40\n",
    "#The inputs and the outputs are offset by 1 character\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "text_chunks = [text_encoded[i:i+chunk_size] for i in range(len(text_encoded) - chunk_size + 1)]\n",
    "\n",
    "# text_chunks[:2]\n",
    "for seq in text_chunks[:1]:\n",
    "    input_seq = seq[:seq_length]\n",
    "    target = seq[seq_length] \n",
    "    print(input_seq, ' -> ', target)\n",
    "    print(repr(''.join(char_array[input_seq])), \n",
    "          ' -> ', repr(''.join(char_array[target])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "        # return torch.tensor(text_chunk[:-1]).long(), torch.tensor(text_chunk[1:]).long()\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(np.array(text_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      " Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    print(' Input (x):', repr(''.join(char_array[seq])))\n",
    "    print('Target (y):', repr(''.join(char_array[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Character-Level RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(80, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_state):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_hidden_size = rnn_hidden_state\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_state, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_state, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        #unsqueeze is done to get the expected input of an RNN (batch_size, 1, sequence_length, embed_dim) -> if its bi-directional or not\n",
    "        out = self.embedding(x).unsqueeze(dim=1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    #Initialisation of hidden state\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Reason why its batch_size x rnn_hidden_size is because for each sequence in the batch, the rnn will process it in parallel\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)\n",
    "    \n",
    "vocab_size = len(char_array)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.3720\n",
      "Epoch 500 loss: 1.5515\n",
      "Epoch 1000 loss: 1.3341\n",
      "Epoch 1500 loss: 1.2545\n",
      "Epoch 2000 loss: 1.2063\n",
      "Epoch 2500 loss: 1.1517\n",
      "Epoch 3000 loss: 1.2063\n",
      "Epoch 3500 loss: 1.1560\n",
      "Epoch 4000 loss: 1.0666\n",
      "Epoch 4500 loss: 1.1005\n",
      "Epoch 5000 loss: 1.1027\n",
      "Epoch 5500 loss: 1.0969\n",
      "Epoch 6000 loss: 1.1341\n",
      "Epoch 6500 loss: 1.0935\n",
      "Epoch 7000 loss: 1.0429\n",
      "Epoch 7500 loss: 1.0514\n",
      "Epoch 8000 loss: 1.0266\n",
      "Epoch 8500 loss: 1.0507\n",
      "Epoch 9000 loss: 1.0259\n",
      "Epoch 9500 loss: 1.0404\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size=batch_size)\n",
    "    #Choose a random batch in DataLoader\n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    \n",
    "    seq_batch = seq_batch.to(device=device)\n",
    "    target_batch = target_batch.to(device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    #RNN loop\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss = loss.item() / seq_length\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch} loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Instead of selecting the character with the highest probabiliy from the softmax all the time, We can randomly sample from the probabilities. More randomous to the output.\n",
    "\n",
    "This can be done using `torch.distributions.categorical.Categorical`, which we can use to draw random samples from a categorical distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.33333334 0.33333334 0.33333334]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "torch.manual_seed(1)\n",
    "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
    "\n",
    "print('Probabilities:', torch.softmax(logits, dim=1).numpy()[0])\n",
    "\n",
    "# Generate a disturbiton based on the logits and outputs values with that probabilies\n",
    "# If using a large sample size, the output of the highest probability will be more compared to the rest\n",
    "m = Categorical(logits=logits)\n",
    "samples = m.sample((10,))\n",
    "\n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.10650698 0.10650698 0.78698605]\n",
      "[[0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
    "\n",
    "m = Categorical(logits=logits)\n",
    "samples = m.sample((10,))\n",
    " \n",
    "print(samples.numpy())\n",
    "#More of 2 than the other categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating\n",
    "\n",
    "The process of consuming the generated sequence as input for generating new elements is called autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island had been manifested,” said the reporter, “do you think the mystery then managed to opprody one in the ore, which\n",
      "bent outside requiving. If it had completely delighted at least two or seven feet, were\n",
      "attentively on the Pacific as possible, and he heard had taken to the\n",
      "part of the streample. Twinhing passages and fulling for the passage.\n",
      "\n",
      "For the night they were impossible cold round, and he struggled without deep six feet long, and friend, had taken with parates covered with one elements, and\n"
     ]
    }
   ],
   "source": [
    "def sample(model, starting_str, \n",
    "           len_generated_text=500, \n",
    "           scale_factor=1.0):\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    hidden = hidden.to('cpu')\n",
    "    cell = cell.to('cpu')\n",
    "    \n",
    "    #Updating the current hidden state from starting_str\n",
    "    for c in range(len(starting_str)-1):\n",
    "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) \n",
    "    \n",
    "    #Generation\n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell) \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        # To have more control, higher the scaling factor, results in more entropy or randomness versus more predictable behavior at lower temperature.\n",
    "        # a < 1, probabilities computed by  a softmax becomes more uniform\n",
    "        scaled_logits = logits * scale_factor\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_array[last_char])\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model.to('cpu')\n",
    "print(sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy man had employed, and no obstacle and gazing and himself, and that part of their\n",
      "height of despatch, formed of cornara the stone-put the point of six\n",
      "coming again, he carried on a series. From this bottle one\n",
      "of Captain Grant’s under the trees at the island, and in by means, as soon as those of the balloon, fell on the beach of the banks of\n",
      "zinc, Lincoln Island to him his suffered.\n",
      "\n",
      "“The ‘Duncan’.” The engineer made to known the colonists who had the blocks of the terrible storm: full-lower pard\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='Happy '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas were not doing to the\n",
      "mist pirate, from thence the absence of the Mercy, the round had slightly dainer, dempatienced!\n",
      "\n",
      "The corral: which are heard. It might have landed he had into the hundred feet from his winghbot. Provisions were graid along\n",
      "the vessel on the ship of March, and if an hour, and oncamers, stone has found at the distance of eruption? No! I crus Harding, deposition,” he exclaimed, “might now about\n",
      "the beach.\n",
      "\n",
      "It was discovered away in the boat, and suddenly hands of the transmire\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='Merry Christmas '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "In natural language processing, when generating text using a language model, there is often a trade-off between producing text that is highly fluent and text that is highly creative. One way to control this trade-off is to adjust the \"temperature\" of the sampling process.\n",
    "\n",
    "The temperature parameter essentially controls the level of randomness in the text generation process. A higher temperature will result in more creative but less fluent text, while a lower temperature will result in less creative but more fluent text.\n",
    "\n",
    "The scale_factor parameter in this function is used to adjust the temperature of the sampling process by scaling the logits (the output of the model before the softmax activation function is applied) before sampling from them. Specifically, if the scale_factor is set to a value greater than 1, it will increase the temperature, and if it is set to a value less than 1, it will decrease the temperature.\n",
    "\n",
    "So, for example, if scale_factor=0.5, then the logits are multiplied by 0.5, which makes the softmax distribution \"sharper\", resulting in a lower temperature and more conservative text generation. On the other hand, if scale_factor=1.5, then the logits are multiplied by 1.5, which makes the softmax distribution \"softer\", resulting in a higher temperature and more creative text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities before scaling:         [0.10650698 0.10650698 0.78698605]\n",
      "Probabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611686]\n",
      "Probabilities after scaling with 0.1: [0.3104238  0.3104238  0.37915248]\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities before scaling:        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
    "print('Probabilities after scaling with 0.5:', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
    "print('Probabilities after scaling with 0.1:', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy many part of the coast, they were obliged to be seen.\n",
      "\n",
      "At this moment the colonists showed him at a distance of the opinion, and as they could not be more than a few steps had been discovered at the state of the forest. An immense movement\n",
      "to the beach and animals and stranger, and proceeded to the stranger, who had thrown at the situation of the surface of the world, and became\n",
      "surprised the engineer.\n",
      "\n",
      "All three miles were obstacles which he had not at the same time also the sand, and the sailo\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='Happy ', scale_factor=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy and I\n",
      "ammedotical presentiment?”\n",
      "\n",
      "“Consequently, he began to run us!”\n",
      "\n",
      "Pencroft and the rising passenies, 3 apes from the fat, which he wished to destroy as for a few days would have exposed from\n",
      "the shore.\n",
      "\n",
      "From the name of Captain Nemo, and also the needles returned Neb.”\n",
      "\n",
      "“The occause of Curable part of this favor of a short bank, because had become kabor. I hope may arrived for impressed forests. It was closed with wetted for the end.\n",
      "\n",
      "It was of everything.”\n",
      "\n",
      "“Pencroft?”\n",
      "\n",
      "As some week of Apr\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, starting_str='Happy ', scale_factor=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "87998d4b2a08f200e53bd85c2d7b01bf2b69127f748e5c1e9a18138bae014c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
